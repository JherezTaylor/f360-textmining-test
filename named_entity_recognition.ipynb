{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "### Logic for identifying person names and occurrence frequencies from a given Chinese wikipedia page\n",
    "\n",
    "\n",
    "'Names of persons' might occur in the same page by complete, incomplete, and alias representations. \n",
    "\n",
    "Examples: 'Donald Trump' and 'Trump'; and 'Ing-Wen Tsai', '蔡英文', '小英', '英文', '蔡博士', '蔡總統', '蔡女士', and '小英總統'.\n",
    "\n",
    "\n",
    "#### Entry\n",
    "\n",
    "How can we improve the precision and recall by filtering 'nouns which are not the name of a person' and 'incorrect names of persons created by segmentation'?\n",
    "\n",
    "#### Advanced \n",
    "\n",
    "How can we find synonyms? For instance, '蔡英文', '小英', '英文', '蔡博士', '蔡總統', '蔡女士', and '小英總統' are 'Ing-Wen Tsai'.\n",
    "\n",
    "#### Challenge \n",
    "\n",
    "How can we distinguish the names of persons from ambiguous results? For instance, 'Mr. Smith is a smith.' in which the second 'smith' is not the name of a person. Another example (in Chinese) is '蔡英文的英文很好', in which the first '蔡英文' is 'Ing-Wen Tsai' but the second '英文' is 'English' which is not the name of a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import datetime\n",
    "import wikipedia\n",
    "import wptools\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import pynlpir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry Level\n",
    "\n",
    "How can we improve the precision and recall by filtering 'nouns which are not the name of a person' and 'incorrect names of persons created by segmentation'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init helper functions\n",
    "\n",
    "I introduce another Chinese segmentation libray [`pynlpir`](http://pynlpir.readthedocs.io/en/latest/index.html) as it offers detailed part of speech tags that are required to solve the challneges. My anecdotal experience is that [`jieba`](https://github.com/fxsjy/jieba) does a better job of segmentation but the drawback is that it doesn't give us the POS tags that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tags can be referenced [here.](http://pynlpir.readthedocs.io/en/latest/pos_map.html?highlight=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba_POS_lookup = set([\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"eng\"])\n",
    "pynlpir_POS_lookup = set([\"noun\",\"personal name\",\"Chinese surname\",\"Chinese given name\",\"Japanese personal name\",\n",
    "                         \"transcribed personal name\",\"toponym\",\"other proper noun\"])\n",
    "\n",
    "POS_MAP = {\"n\":\"noun\",\"nr\":\"personal name\",\"nr1\":\"Chinese surname\",\"nr2\":\"Chinese given name\",\"nrf\":\"transcribed personal name\",\n",
    "           \"nrj\":\"Japanese personal name\",\"nz\":\"other proper noun\",\"ns\":\"toponym\"\n",
    "          }\n",
    "\n",
    "inv_POS_MAP = inv_map = {v: k for k, v in POS_MAP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init and compile regex\n",
    "\n",
    "I define several regex patterns to determine if a given sequence of characters should be collapsed and considered as one word. While not 100% accurate, I came up with this logic by looking at the parse results for several sentences and examples. \n",
    "\n",
    "Examples of these patterns are:\n",
    "- surname, proper noun ex: 蔡英文\n",
    "- surname, noun, noun ex: 蔡博士\n",
    "- surname, transcibed noun ex: 唐納川普\n",
    "- given name, noun, noun ex: 小英總統\n",
    "\n",
    "The results seem to hold up but ultimately, my Chinese is not native so I am limited in that regard. With more domain knowledge more patterns and filters can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surname_proper_noun = r\"\\b(?P<index_1>\\d{1,4})_(?P<pos_tag_1>nr1)(?:\\s)(?P<index_2>\\d{1,4})_(?P<pos_tag_2>nz|nr2)(?:\\s)\\b\"\n",
    "\n",
    "surname_noun_noun = r\"\\b(?P<index_1>\\d{1,4})_(?P<pos_tag_1>nr1)(?:\\s)(?P<index_2>\\d{1,4})_(?P<pos_tag_2>n)(?:\\s)(?P<index_3>\\d{1,4})?_?(?P<pos_tag_3>n)?\\b\"\n",
    "\n",
    "given_name_noun_noun = r\"\\b(?P<index_1>\\d{1,4})_(?P<pos_tag_1>nr2)(?:\\s)(?P<index_2>\\d{1,4})_(?P<pos_tag_2>n)(?:\\s)(?P<index_3>\\d{1,4})?_?(?P<pos_tag_3>n)\\b\"\n",
    "\n",
    "surname_transcribed_noun = r\"\\b(?P<index_1>\\d{1,4})_(?P<pos_tag_1>nr1)(?:\\s)(?P<index_2>\\d{1,4})_(?P<pos_tag_2>n)(?:\\s)(?P<index_3>\\d{1,4})?_?(?P<pos_tag_3>nrf)\\b\"\n",
    "\n",
    "cmp_surname_proper_noun = re.compile(surname_proper_noun)\n",
    "cmp_surname_noun_noun = re.compile(surname_noun_noun)\n",
    "cmp_given_name_noun_noun = re.compile(given_name_noun_noun)\n",
    "cmp_surname_transcribed_noun = re.compile(surname_transcribed_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct segmentation and extract person names\n",
    "\n",
    "The above patterns take precedence and are used to extract the first batch of names from the page and correct\n",
    "segmentation issues, removing any token that was seen from the final result set to avoid duplication. The text is parsed and tagged with its given part of speech (POS), retaining all tagged as personal nouns.\n",
    "\n",
    "Finally, the remaining words tagged as person nouns are returned along with the collapsed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_nouns(page_text):\n",
    "    \"\"\" Parse all the person names from the given page text.\n",
    "    \n",
    "    A few regex rules are defined to correct incorrect names created by segmentation.\n",
    "    These rules are processed first, removing any token that was seen from the final\n",
    "    result set to avoid duplication. \n",
    "    \n",
    "    Finally, the remaining words tagged as person nouns are returned along with the collapsed\n",
    "    words.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Wikipedia page text.\n",
    "    \n",
    "    Returns:\n",
    "        person_names (list): List of person names parsed from the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we initialize the API and filter the pos tags we want to keep\n",
    "    pynlpir.open()\n",
    "    pos_tags = pynlpir.segment(page_text, pos_names='child')\n",
    "    pos_tags = [item for item in pos_tags if item[1] in pynlpir_POS_lookup]\n",
    "    \n",
    "    # In order to collapse words, we create a condensed string that we will check\n",
    "    # against our regex patterns for segmentation correction\n",
    "    condensed_pos_list = [str(idx) + \"_\" + inv_POS_MAP[item[1]] \\\n",
    "                         for idx, item in enumerate(pos_tags)]\n",
    "    condensed_pos_str = \" \".join(condensed_pos_list)\n",
    "    pynlpir.close()\n",
    "    \n",
    "    surname_proper_noun_list = list(cmp_surname_proper_noun.finditer(condensed_pos_str))\n",
    "    surname_noun_noun_list = list(cmp_surname_noun_noun.finditer(condensed_pos_str))\n",
    "    given_name_noun_noun_list = list(cmp_given_name_noun_noun.finditer(condensed_pos_str))\n",
    "    surname_transcribed_noun_list = list(cmp_surname_transcribed_noun.finditer(condensed_pos_str))\n",
    "    \n",
    "    # Merge all the lists for processing\n",
    "    match_object_list = surname_proper_noun_list + surname_noun_noun_list \\\n",
    "                        + given_name_noun_noun_list + surname_transcribed_noun_list\n",
    "    \n",
    "    person_names, seen_indices = process_match_list(pos_tags, match_object_list)\n",
    "    \n",
    "    # We create a set out of the indices we have seen\n",
    "    # Tokens positions appearing as part of multiple words are not considered\n",
    "    # We also limit the results to characters above 1 in length\n",
    "    seen_indices = set(seen_indices)\n",
    "    for idx, item in enumerate(pos_tags):\n",
    "        if idx not in seen_indices and len(item[0]) > 1:\n",
    "            person_names.append(item[0])\n",
    "    \n",
    "    return person_names\n",
    "\n",
    "def process_match_list(pos_tags, match_object_list):\n",
    "    \"\"\" Process the stored match objects and extract\n",
    "    the collapsed words at each index.\n",
    "    \n",
    "    Args:\n",
    "        pos_tags (list): List of word:tag tuples.\n",
    "        match_object_list (list): List of regex match objects.\n",
    "    \n",
    "    Returns\n",
    "        collapsed_list(list), seen_indices(list)\n",
    "    \"\"\"\n",
    "    \n",
    "    collapsed_list = []\n",
    "    seen_indices = []\n",
    "    for item in match_object_list:\n",
    "        result, seen = collapse_words(pos_tags,item)\n",
    "        collapsed_list.append(result)\n",
    "        seen_indices = seen_indices + seen\n",
    "    return collapsed_list, seen_indices\n",
    "    \n",
    "def collapse_words(pos_list, match_object):\n",
    "    \"\"\" Use the result of the regex patterns and \n",
    "    collapse the words into single units. This attempts\n",
    "    to correct incorrect names created by segmentation.\n",
    "    \n",
    "    Args:\n",
    "        pos_list (list): List of word:POS tuples.\n",
    "        match_object: (regex match object)\n",
    "    \n",
    "    Returns:\n",
    "        collapsed(str), seen_indices(list)\n",
    "    \"\"\"\n",
    "    \n",
    "    seen_indices = []\n",
    "    collapsed = \"\"\n",
    "    group_str = \"index_\"\n",
    "    for idx in range(1,4):\n",
    "        curr = group_str + str(idx)\n",
    "        try:\n",
    "            char_position = int(match_object.group(curr))\n",
    "            character = pos_list[char_position][0]\n",
    "            collapsed += character\n",
    "            seen_indices.append(char_position)\n",
    "        except:\n",
    "            pass\n",
    "    return collapsed, seen_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The English double tap\n",
    "\n",
    "While the above code can get us most of what we need for Chinese words, English is another issue. We therefore need to do a secondary pass to extract English tokens and get the entity that they represent. We need to use an English Named Entity Recognition library. I have experience with the NLP toolkit [spaCy](https://spacy.io/) so I'll use that. I keep the function modular in case we need to extract entities of a different type later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def get_eng_tokens(page_text):\n",
    "    \"\"\" Parse all the English tokens from the page text.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Wikipedia page text.\n",
    "    \n",
    "    Returns:\n",
    "        eng_tokens (list): List of tokens tagged as English.\n",
    "    \"\"\"\n",
    "    \n",
    "    # topK refers to the number of tags to return, based on tf-idf\n",
    "    # Set this to an arbitrarily high number to get all words\n",
    "    tags = jieba.analyse.extract_tags(page_text, topK=50000)\n",
    "    filtered_tags = [tag for tag in tags if not tag.isnumeric() and len(tag) >= 2]\n",
    "    filtered_string = \", \".join(filtered_tags)\n",
    "    \n",
    "    token_tags = list(pseg.cut(filtered_string))\n",
    "    return [token.word for token in token_tags if token.flag == \"eng\"]\n",
    "\n",
    "def get_named_entities(page_text, entity=\"PERSON\"):\n",
    "    \"\"\" Filter English tokens by entity, defaults to PERSON.\n",
    "    \n",
    "    We want to retain all tokens that are tagged as a given entity.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Wikipedia page text.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_names = []\n",
    "    eng_tokens = get_eng_tokens(page_text)\n",
    "    \n",
    "    for token in eng_tokens:\n",
    "        doc = nlp(token)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity:\n",
    "                entity_names.append(ent.text)\n",
    "    \n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init page url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set wikipedia api language\n",
    "lang = \"zh\"\n",
    "wikipedia.set_lang(lang)\n",
    "\n",
    "# Here I declare the title of the zh wikipedia page, formatted as https://zh.wikipedia.org/wiki/<page_title>\n",
    "page_title = \"唐納·川普\"\n",
    "page_text = wikipedia.page(page_title).content\n",
    "\n",
    "# Strip away new lines\n",
    "page_text = page_text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry Level Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('蔡', 'Chinese surname'),\n",
       " ('英文', 'other proper noun'),\n",
       " ('小英', 'Chinese given name'),\n",
       " ('英文', 'other proper noun'),\n",
       " ('蔡', 'Chinese surname'),\n",
       " ('博士', 'noun'),\n",
       " ('蔡', 'Chinese surname'),\n",
       " ('總', 'noun')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"蔡英文', '小英', '英文', '蔡博士', '蔡總統', '蔡女士', and '小英總統, Donald Trump, 唐納·川普\"\n",
    "pynlpir.open()\n",
    "pos_tags = pynlpir.segment(test_str, pos_names='child')\n",
    "pos_tags = [item for item in pos_tags if item[1] in pynlpir_POS_lookup]\n",
    "pynlpir.close()\n",
    "pos_tags[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default segmenter, words like 蔡英文 and 蔡總統 become split into their component characters. However, we can learn some rules that let us know how to correct these issues as seen in the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蔡英文', '蔡博士', '蔡總統', '蔡女士', '唐納', '小英總統', '唐納川普', '小英', '英文', 'Donald', 'Trump']\n"
     ]
    }
   ],
   "source": [
    "print(extract_nouns(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract English tokens first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.911 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "eng_tokens = get_eng_tokens(page_text)\n",
    "eng_token_set = set(eng_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get English names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_names = get_named_entities(page_text, \"PERSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a set of tokens that we don't want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_blacklist = eng_token_set.difference(set(english_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entry Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "part of speech not recognized: 'gjtgj'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n",
      "part of speech not recognized: 'gms'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('特朗普', 84),\n",
      " ('川普', 48),\n",
      " ('移民', 25),\n",
      " ('共和', 24),\n",
      " ('总统', 13),\n",
      " ('政策', 11),\n",
      " ('公司', 11),\n",
      " ('英文', 10),\n",
      " ('穆斯林', 10),\n",
      " ('世界', 9),\n",
      " ('小姐', 9),\n",
      " ('女性', 8),\n",
      " ('媒体', 7),\n",
      " ('命令', 7),\n",
      " ('行政', 6),\n",
      " ('对手', 6),\n",
      " ('弗雷德', 6),\n",
      " ('代表', 6),\n",
      " ('政治', 6),\n",
      " ('克林顿', 5),\n",
      " ('白人', 5),\n",
      " ('候选人', 5),\n",
      " ('法院', 5),\n",
      " ('名人', 5),\n",
      " ('利益', 5),\n",
      " ('家庭', 4),\n",
      " ('子女', 4),\n",
      " ('部分', 4),\n",
      " ('政府', 4),\n",
      " ('共和党', 4),\n",
      " ('公民', 4),\n",
      " ('柯林', 4),\n",
      " ('时报', 4),\n",
      " ('皇后', 4),\n",
      " ('选票', 3),\n",
      " ('Trump', 3),\n",
      " ('社交', 3),\n",
      " ('言论', 3),\n",
      " ('法案', 3),\n",
      " ('生意', 3),\n",
      " ('委员会', 3),\n",
      " ('节目', 3),\n",
      " ('同盟', 3),\n",
      " ('选民', 3),\n",
      " ('关系', 3),\n",
      " ('火星', 3),\n",
      " ('故事', 3),\n",
      " ('事件', 3),\n",
      " ('中国', 3),\n",
      " ('棒球', 3),\n",
      " ('工人', 3),\n",
      " ('盟友', 3),\n",
      " ('联邦', 3),\n",
      " ('希拉蕊', 3),\n",
      " ('公寓', 3),\n",
      " ('民主党', 3),\n",
      " ('詹姆斯·科米', 3),\n",
      " ('气候', 3),\n",
      " ('历史', 3),\n",
      " ('电视', 3),\n",
      " ('主席', 3),\n",
      " ('人物', 3),\n",
      " ('房地产', 2),\n",
      " ('婚姻', 2),\n",
      " ('选手', 2),\n",
      " ('梅拉尼娅', 2),\n",
      " ('总体', 2),\n",
      " ('父母', 2),\n",
      " ('手段', 2),\n",
      " ('公职', 2),\n",
      " ('方式', 2),\n",
      " ('全国', 2),\n",
      " ('核心', 2),\n",
      " ('人口', 2),\n",
      " ('色彩', 2),\n",
      " ('官员', 2),\n",
      " ('商业', 2),\n",
      " ('形象', 2),\n",
      " ('庄园', 2),\n",
      " ('孩子', 2),\n",
      " ('外界', 2),\n",
      " ('唐纳德·特朗普', 2),\n",
      " ('民族主义', 2),\n",
      " ('房地', 2),\n",
      " ('国家', 2),\n",
      " ('大法官', 2),\n",
      " ('备忘录', 2),\n",
      " ('修正案', 2),\n",
      " ('近平', 2),\n",
      " ('信息', 2),\n",
      " ('家族', 2),\n",
      " ('范围', 2),\n",
      " ('博士', 2),\n",
      " ('康尼', 2),\n",
      " ('措施', 2),\n",
      " ('票数', 2),\n",
      " ('主持人', 2),\n",
      " ('被害人', 2),\n",
      " ('逆差', 2),\n",
      " ('习近平', 2),\n",
      " ('大麻', 2),\n",
      " ('周刊', 2),\n",
      " ('调查局', 2),\n",
      " ('朋友', 2),\n",
      " ('科米', 2),\n",
      " ('支持者', 2),\n",
      " ('大学', 2),\n",
      " ('更衣室', 2),\n",
      " ('政权', 2),\n",
      " ('问题', 2),\n",
      " ('男性', 2),\n",
      " ('合约', 2),\n",
      " ('年代', 2),\n",
      " ('福坦莫', 2),\n",
      " ('理由', 2),\n",
      " ('外交', 2),\n",
      " ('男人', 2),\n",
      " ('法官', 2),\n",
      " ('人士', 2),\n",
      " ('布希', 2),\n",
      " ('伊凡娜·特朗普', 2),\n",
      " ('希拉里·克林顿', 2),\n",
      " ('祖父', 1),\n",
      " ('川普商', 1),\n",
      " ('学者', 1),\n",
      " ('太平洋', 1),\n",
      " ('站台', 1),\n",
      " ('後墨活', 1),\n",
      " ('福利', 1),\n",
      " ('航空', 1),\n",
      " ('小丘', 1),\n",
      " ('计划', 1),\n",
      " ('最高法院', 1),\n",
      " ('科学', 1),\n",
      " ('明星', 1),\n",
      " ('项目', 1),\n",
      " ('主流', 1),\n",
      " ('评论家', 1),\n",
      " ('错误', 1),\n",
      " ('於理官', 1),\n",
      " ('里斯', 1),\n",
      " ('保守主义', 1),\n",
      " ('助理', 1),\n",
      " ('商人', 1),\n",
      " ('情境', 1),\n",
      " ('意愿', 1),\n",
      " ('玛拉·梅普尔', 1),\n",
      " ('宣人身份', 1),\n",
      " ('数字', 1),\n",
      " ('经历', 1),\n",
      " ('於理性生命', 1),\n",
      " ('保姆', 1),\n",
      " ('夫妻', 1),\n",
      " ('机构', 1),\n",
      " ('迪表', 1),\n",
      " ('弗雷德·特朗普', 1),\n",
      " ('後行程行政', 1),\n",
      " ('福布斯', 1),\n",
      " ('Elizabeth', 1),\n",
      " ('森林', 1),\n",
      " ('位置', 1),\n",
      " ('邱林', 1),\n",
      " ('特朗普因', 1),\n",
      " ('定语', 1),\n",
      " ('後TPP主', 1),\n",
      " ('於共和值', 1),\n",
      " ('Bennett', 1),\n",
      " ('概念', 1),\n",
      " ('後事情沙', 1),\n",
      " ('弗林', 1),\n",
      " ('材料', 1),\n",
      " ('人员', 1),\n",
      " ('净资产', 1),\n",
      " ('毒品', 1),\n",
      " ('於Rascals台', 1),\n",
      " ('资金', 1),\n",
      " ('韦德案', 1),\n",
      " ('唐納川普', 1),\n",
      " ('法律', 1),\n",
      " ('要求', 1),\n",
      " ('福特', 1),\n",
      " ('城市', 1),\n",
      " ('Tony', 1),\n",
      " ('於总统川普', 1),\n",
      " ('後公民', 1),\n",
      " ('重工', 1),\n",
      " ('娱乐', 1),\n",
      " ('蔡英文', 1),\n",
      " ('协定', 1),\n",
      " ('男孩', 1),\n",
      " ('特朗普成', 1),\n",
      " ('航母', 1),\n",
      " ('道德', 1),\n",
      " ('后裔', 1),\n",
      " ('冠夫', 1),\n",
      " ('言人保', 1),\n",
      " ('於人梅拉尼娅·特朗普', 1),\n",
      " ('排行榜', 1),\n",
      " ('美元', 1),\n",
      " ('政治家', 1),\n",
      " ('大使', 1),\n",
      " ('姿态', 1),\n",
      " ('重量', 1),\n",
      " ('人才', 1),\n",
      " ('若中步政策', 1),\n",
      " ('民众', 1),\n",
      " ('裁判', 1),\n",
      " ('青少年', 1),\n",
      " ('唐納', 1),\n",
      " ('罗西·奥唐奈', 1),\n",
      " ('太空船', 1),\n",
      " ('南海', 1),\n",
      " ('情况', 1),\n",
      " ('外长', 1),\n",
      " ('得超', 1),\n",
      " ('於政策成因', 1),\n",
      " ('生命', 1),\n",
      " ('文本', 1),\n",
      " ('领导人', 1),\n",
      " ('面的', 1),\n",
      " ('眼睛', 1),\n",
      " ('派系', 1),\n",
      " ('方向', 1),\n",
      " ('精力', 1),\n",
      " ('信心', 1),\n",
      " ('体重', 1),\n",
      " ('来源', 1),\n",
      " ('官司', 1),\n",
      " ('於穆斯林', 1),\n",
      " ('川普公', 1),\n",
      " ('过程', 1),\n",
      " ('方面', 1),\n",
      " ('後影', 1),\n",
      " ('传统', 1),\n",
      " ('报道', 1),\n",
      " ('克·彭斯', 1),\n",
      " ('唐纳德·约翰·特朗普', 1),\n",
      " ('谢尔盖', 1),\n",
      " ('後團給', 1),\n",
      " ('办公室', 1),\n",
      " ('中川', 1),\n",
      " ('於波手', 1),\n",
      " ('年薪', 1),\n",
      " ('後州商', 1),\n",
      " ('富翁', 1),\n",
      " ('霍士新', 1),\n",
      " ('後裁判奥斯汀', 1),\n",
      " ('布什', 1),\n",
      " ('McMahon', 1),\n",
      " ('弗雷德里克·特朗普', 1),\n",
      " ('席位', 1),\n",
      " ('乌玛加', 1),\n",
      " ('商而', 1),\n",
      " ('教授', 1),\n",
      " ('杰西·文图拉', 1),\n",
      " ('埃里克', 1),\n",
      " ('於表印象', 1),\n",
      " ('社会', 1),\n",
      " ('征兆', 1),\n",
      " ('Vince', 1),\n",
      " ('保护主义', 1),\n",
      " ('母均非', 1),\n",
      " ('男方', 1),\n",
      " ('院中', 1),\n",
      " ('梅拉尼娅生', 1),\n",
      " ('例子', 1),\n",
      " ('宣情世界', 1),\n",
      " ('看法', 1),\n",
      " ('源放', 1),\n",
      " ('夫球部分右派', 1),\n",
      " ('沃尔特', 1),\n",
      " ('拉夫罗夫', 1),\n",
      " ('喜事', 1),\n",
      " ('接班人', 1),\n",
      " ('迪表川普', 1),\n",
      " ('後家', 1),\n",
      " ('标志性', 1),\n",
      " ('香港', 1),\n",
      " ('英语', 1),\n",
      " ('川普村', 1),\n",
      " ('之星', 1),\n",
      " ('立场', 1),\n",
      " ('後德理默克尔', 1),\n",
      " ('日美核武核子', 1),\n",
      " ('後妻子三子', 1),\n",
      " ('数量', 1),\n",
      " ('主义', 1),\n",
      " ('天堂', 1),\n",
      " ('E.', 1),\n",
      " ('耻辱', 1),\n",
      " ('祖先', 1),\n",
      " ('现象', 1),\n",
      " ('组织', 1),\n",
      " ('凤凰', 1),\n",
      " ('生涯', 1),\n",
      " ('墨索里尼相', 1),\n",
      " ('限期', 1),\n",
      " ('民粹主义', 1),\n",
      " ('海岸', 1),\n",
      " ('希拉里·克林', 1),\n",
      " ('J·', 1),\n",
      " ('中士', 1),\n",
      " ('工行', 1),\n",
      " ('北美', 1),\n",
      " ('谢尔盖·基斯利亚克', 1),\n",
      " ('富豪', 1),\n",
      " ('超市', 1),\n",
      " ('赌场', 1),\n",
      " ('真人', 1),\n",
      " ('高品', 1),\n",
      " ('暴行', 1),\n",
      " ('後序幕', 1),\n",
      " ('芭芭', 1),\n",
      " ('王子', 1),\n",
      " ('说辞', 1),\n",
      " ('细节', 1),\n",
      " ('Donald', 1),\n",
      " ('星光', 1),\n",
      " ('於星星主席', 1),\n",
      " ('信徒', 1),\n",
      " ('主角', 1),\n",
      " ('情报', 1),\n",
      " ('日子', 1),\n",
      " ('Kelly', 1),\n",
      " ('克郡地', 1),\n",
      " ('希特勒', 1),\n",
      " ('学院', 1),\n",
      " ('州长', 1),\n",
      " ('理念', 1),\n",
      " ('民意', 1),\n",
      " ('鲍比·拉什利', 1),\n",
      " ('母亲', 1),\n",
      " ('致美白人部分', 1),\n",
      " ('特例', 1),\n",
      " ('大道', 1),\n",
      " ('搭档', 1),\n",
      " ('明尼', 1),\n",
      " ('辛民主党任期', 1),\n",
      " ('V·', 1),\n",
      " ('时间', 1),\n",
      " ('念日行技巧', 1),\n",
      " ('合法性', 1),\n",
      " ('足球', 1),\n",
      " ('玛丽安娜·特朗普·巴里', 1),\n",
      " ('迪·朱利安尼', 1),\n",
      " ('富可', 1),\n",
      " ('期限', 1),\n",
      " ('日美', 1),\n",
      " ('互联网', 1),\n",
      " ('交易', 1),\n",
      " ('将军', 1),\n",
      " ('环境', 1),\n",
      " ('兴趣', 1),\n",
      " ('玩笑', 1),\n",
      " ('小唐纳德', 1),\n",
      " ('小鬼', 1),\n",
      " ('後政策Honor', 1),\n",
      " ('於人', 1),\n",
      " ('预算', 1),\n",
      " ('情节', 1),\n",
      " ('全世界', 1),\n",
      " ('禁令', 1),\n",
      " ('住宅', 1),\n",
      " ('女婿', 1),\n",
      " ('能源', 1),\n",
      " ('档案馆', 1),\n",
      " ('彭更衣室更衣室', 1),\n",
      " ('容展中方美金', 1),\n",
      " ('成威', 1),\n",
      " ('好人', 1),\n",
      " ('後公寓房', 1),\n",
      " ('後裁判', 1),\n",
      " ('酒店', 1),\n",
      " ('地方', 1),\n",
      " ('葛利共和', 1),\n",
      " ('观点', 1),\n",
      " ('後横财', 1),\n",
      " ('泰特', 1),\n",
      " ('己方', 1),\n",
      " ('骗局', 1),\n",
      " ('後G7德', 1),\n",
      " ('死刑', 1),\n",
      " ('股份', 1),\n",
      " ('木匠', 1),\n",
      " ('後他用', 1),\n",
      " ('注释', 1),\n",
      " ('甄方法特朗普', 1),\n",
      " ('後進房', 1),\n",
      " ('於总统', 1),\n",
      " ('於影片', 1),\n",
      " ('迈克·彭斯', 1),\n",
      " ('卡利', 1),\n",
      " ('照片', 1),\n",
      " ('生理', 1),\n",
      " ('沙特', 1),\n",
      " ('兄弟', 1),\n",
      " ('北京', 1),\n",
      " ('经济', 1),\n",
      " ('塔拉·康纳', 1),\n",
      " ('力花事人', 1),\n",
      " ('教徒', 1),\n",
      " ('约翰·麦凯恩', 1),\n",
      " ('得明', 1),\n",
      " ('後房地生意', 1),\n",
      " ('福斯', 1),\n",
      " ('首都', 1),\n",
      " ('金融', 1),\n",
      " ('葛文达·布莱尔', 1),\n",
      " ('後球市', 1),\n",
      " ('长城', 1),\n",
      " ('成效', 1),\n",
      " ('Rosie', 1),\n",
      " ('上尉', 1),\n",
      " ('生日', 1),\n",
      " ('输赢', 1),\n",
      " ('百分点', 1),\n",
      " ('普企', 1),\n",
      " ('保守派', 1),\n",
      " ('国际', 1),\n",
      " ('Conway', 1),\n",
      " ('冷石', 1),\n",
      " ('对方', 1),\n",
      " ('记者', 1),\n",
      " ('宗教', 1),\n",
      " ('特朗普泰姬', 1),\n",
      " ('广义', 1),\n",
      " ('Bobby', 1),\n",
      " ('尼尔·戈萨奇', 1),\n",
      " ('人家', 1),\n",
      " ('家具', 1),\n",
      " ('康威', 1),\n",
      " ('程有', 1),\n",
      " ('个人', 1),\n",
      " ('仇恨', 1),\n",
      " ('後家川普', 1),\n",
      " ('军职', 1),\n",
      " ('模式', 1),\n",
      " ('波特', 1),\n",
      " ('蓝色', 1),\n",
      " ('教堂', 1),\n",
      " ('成少', 1),\n",
      " ('身份', 1),\n",
      " ('目的', 1),\n",
      " ('伯父', 1),\n",
      " ('台词', 1),\n",
      " ('堂弟', 1),\n",
      " ('争议性', 1),\n",
      " ('史密斯', 1),\n",
      " ('伊凡娜·泽尔尼茨科娃', 1),\n",
      " ('中华人民共和国', 1),\n",
      " ('新闻', 1),\n",
      " ('小时', 1),\n",
      " ('恐怖主义', 1),\n",
      " ('支票', 1),\n",
      " ('托尼·班尼特', 1),\n",
      " ('比利·布希', 1),\n",
      " ('天主教', 1),\n",
      " ('效果', 1),\n",
      " ('甄方法', 1),\n",
      " ('John', 1),\n",
      " ('歌手', 1),\n",
      " ('董事长', 1),\n",
      " ('日报', 1),\n",
      " ('主力', 1),\n",
      " ('结果', 1),\n",
      " ('波音', 1),\n",
      " ('品牌', 1),\n",
      " ('共识', 1),\n",
      " ('高调', 1),\n",
      " ('椭圆形', 1),\n",
      " ('局长', 1),\n",
      " ('姊妹', 1),\n",
      " ('月球', 1),\n",
      " ('裔移民福利', 1),\n",
      " ('健美', 1),\n",
      " ('唐納德', 1),\n",
      " ('人生', 1),\n",
      " ('邮报', 1),\n",
      " ('影片', 1),\n",
      " ('火箭', 1),\n",
      " ('原文', 1),\n",
      " ('官方', 1),\n",
      " ('後序幕特朗普', 1),\n",
      " ('墨西哥人', 1),\n",
      " ('宣目的', 1),\n",
      " ('於大道', 1),\n",
      " ('妻子', 1),\n",
      " ('後德', 1),\n",
      " ('索尼', 1),\n",
      " ('客人', 1),\n",
      " ('精英', 1)]\n"
     ]
    }
   ],
   "source": [
    "page_nouns = extract_nouns(page_text)\n",
    "word_frequencies = Counter()\n",
    "for word in page_nouns:\n",
    "    if word not in token_blacklist:\n",
    "        word_frequencies[word] += 1\n",
    "        \n",
    "pprint(word_frequencies.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 491\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count: \" + str(len(word_frequencies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One last filter\n",
    "\n",
    "The initial results look messy, we can further refine the list since we don't need to do any more segmentation correction. Let's define the POS tags we want to keep at the end. Here we will try to remove all nouns that remain after trying to segment a word.\n",
    "\n",
    "I reason that if a word is segmented and it does not have a POS tag we want to keep then it should be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_POS_lookup = set([\"personal name\",\"Chinese surname\",\"Chinese given name\",\"Japanese personal name\",\n",
    "                         \"transcribed personal name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_tokens(original_frequencies, english_names):\n",
    "    \"\"\" Restrict the words to those with a POS we want to retain\n",
    "    and those that do not break into characters upon individual\n",
    "    segmentation.\n",
    "    \n",
    "    Args:\n",
    "        original_frequencies (Counter): Counter of words.\n",
    "        english_names (set): Set of English names to retain.\n",
    "        \n",
    "    Return:\n",
    "        word_frequencies (Counter): Filtered words.\n",
    "    \"\"\"\n",
    "    \n",
    "    pynlpir.open()\n",
    "    \n",
    "    # Create a copy to avoid altering the orignal Counter\n",
    "    word_frequencies = copy.deepcopy(original_frequencies)\n",
    "    token_set = set(word_frequencies)\n",
    "    \n",
    "    for token in token_set:\n",
    "        # Keep all of these\n",
    "        if token in english_names:\n",
    "            pass\n",
    "        \n",
    "        # Lets test to see if the word breaks into smaller characters\n",
    "        # Most Chinese names are either 2 or 1 characters in length.\n",
    "        elif len(token) <= 3:\n",
    "            pos_tag = pynlpir.segment(token, pos_names='child')\n",
    "            \n",
    "            # The token has been broken into smaller characters, check if it\n",
    "            # should be discarded. Our patterns take the form of a Surname character\n",
    "            # followed by some other noun character\n",
    "            \n",
    "            if len(pos_tag) > 1:\n",
    "                if len(pos_tag) == 2 and pos_tag[0][1] == \"Chinese surname\" and pos_tag[1][1] in pynlpir_POS_lookup:\n",
    "                    pass\n",
    "                else:\n",
    "                    del word_frequencies[token]\n",
    "            \n",
    "            # The token remains whole, check the POS\n",
    "            elif pos_tag[0][1] not in final_POS_lookup:\n",
    "                del word_frequencies[token]\n",
    "        \n",
    "        # Check for transcibed names\n",
    "        elif len(token) > 3:\n",
    "            pos_tag = pynlpir.segment(token, pos_names='child')\n",
    "            if len(pos_tag) == 1 and pos_tag[0][1] not in final_POS_lookup:\n",
    "                del word_frequencies[token]\n",
    "    \n",
    "    pynlpir.close()\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised Entry Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "part of speech not recognized: 'gms'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('川普', 48),\n",
      " ('弗雷德', 6),\n",
      " ('克林顿', 5),\n",
      " ('柯林', 4),\n",
      " ('詹姆斯·科米', 3),\n",
      " ('Trump', 3),\n",
      " ('希拉蕊', 3),\n",
      " ('梅拉尼娅', 2),\n",
      " ('唐纳德·特朗普', 2),\n",
      " ('福坦莫', 2),\n",
      " ('康尼', 2),\n",
      " ('伊凡娜·特朗普', 2),\n",
      " ('科米', 2),\n",
      " ('习近平', 2),\n",
      " ('希拉里·克林顿', 2),\n",
      " ('布希', 2),\n",
      " ('川普商', 1),\n",
      " ('史密斯', 1),\n",
      " ('小丘', 1),\n",
      " ('最高法院', 1),\n",
      " ('高品', 1),\n",
      " ('里斯', 1),\n",
      " ('玛拉·梅普尔', 1),\n",
      " ('宣人身份', 1),\n",
      " ('於理性生命', 1),\n",
      " ('迪表', 1),\n",
      " ('弗雷德·特朗普', 1),\n",
      " ('後行程行政', 1),\n",
      " ('福布斯', 1),\n",
      " ('Elizabeth', 1),\n",
      " ('邱林', 1),\n",
      " ('特朗普因', 1),\n",
      " ('後TPP主', 1),\n",
      " ('後横财', 1),\n",
      " ('後事情沙', 1),\n",
      " ('弗林', 1),\n",
      " ('於Rascals台', 1),\n",
      " ('唐納川普', 1),\n",
      " ('Tony', 1),\n",
      " ('於总统川普', 1),\n",
      " ('後公民', 1),\n",
      " ('蔡英文', 1),\n",
      " ('冷石', 1),\n",
      " ('特朗普成', 1),\n",
      " ('韦德案', 1),\n",
      " ('冠夫', 1),\n",
      " ('於人梅拉尼娅·特朗普', 1),\n",
      " ('若中步政策', 1),\n",
      " ('唐納', 1),\n",
      " ('罗西·奥唐奈', 1),\n",
      " ('於穆斯林', 1),\n",
      " ('川普公', 1),\n",
      " ('克·彭斯', 1),\n",
      " ('谢尔盖', 1),\n",
      " ('霍士新', 1),\n",
      " ('後裁判奥斯汀', 1),\n",
      " ('布什', 1),\n",
      " ('於影片', 1),\n",
      " ('乌玛加', 1),\n",
      " ('杰西·文图拉', 1),\n",
      " ('埃里克', 1),\n",
      " ('於表印象', 1),\n",
      " ('後G7德', 1),\n",
      " ('Vince', 1),\n",
      " ('宣情世界', 1),\n",
      " ('夫球部分右派', 1),\n",
      " ('拉夫罗夫', 1),\n",
      " ('迪表川普', 1),\n",
      " ('後家', 1),\n",
      " ('川普村', 1),\n",
      " ('後德理默克尔', 1),\n",
      " ('日美核武核子', 1),\n",
      " ('後妻子三子', 1),\n",
      " ('希拉里·克林', 1),\n",
      " ('谢尔盖·基斯利亚克', 1),\n",
      " ('弗雷德里克·特朗普', 1),\n",
      " ('鲍比·拉什利', 1),\n",
      " ('McMahon', 1),\n",
      " ('芭芭', 1),\n",
      " ('Donald', 1),\n",
      " ('星光', 1),\n",
      " ('於星星主席', 1),\n",
      " ('Kelly', 1),\n",
      " ('希特勒', 1),\n",
      " ('辛民主党任期', 1),\n",
      " ('後家川普', 1),\n",
      " ('念日行技巧', 1),\n",
      " ('玛丽安娜·特朗普·巴里', 1),\n",
      " ('迪·朱利安尼', 1),\n",
      " ('後裁判', 1),\n",
      " ('後球市', 1),\n",
      " ('小唐纳德', 1),\n",
      " ('後政策Honor', 1),\n",
      " ('於人', 1),\n",
      " ('墨索里尼相', 1),\n",
      " ('彭更衣室更衣室', 1),\n",
      " ('容展中方美金', 1),\n",
      " ('成威', 1),\n",
      " ('後公寓房', 1),\n",
      " ('葛利共和', 1),\n",
      " ('Bennett', 1),\n",
      " ('泰特', 1),\n",
      " ('於政策成因', 1),\n",
      " ('甄方法特朗普', 1),\n",
      " ('Rosie', 1),\n",
      " ('於总统', 1),\n",
      " ('迈克·彭斯', 1),\n",
      " ('梅拉尼娅生', 1),\n",
      " ('後序幕', 1),\n",
      " ('约翰·麦凯恩', 1),\n",
      " ('後房地生意', 1),\n",
      " ('福斯', 1),\n",
      " ('葛文达·布莱尔', 1),\n",
      " ('力花事人', 1),\n",
      " ('Conway', 1),\n",
      " ('特朗普泰姬', 1),\n",
      " ('Bobby', 1),\n",
      " ('尼尔·戈萨奇', 1),\n",
      " ('於共和值', 1),\n",
      " ('波特', 1),\n",
      " ('致美白人部分', 1),\n",
      " ('唐纳德·约翰·特朗普', 1),\n",
      " ('後他用', 1),\n",
      " ('伊凡娜·泽尔尼茨科娃', 1),\n",
      " ('母均非', 1),\n",
      " ('源放', 1),\n",
      " ('托尼·班尼特', 1),\n",
      " ('比利·布希', 1),\n",
      " ('甄方法', 1),\n",
      " ('John', 1),\n",
      " ('塔拉·康纳', 1),\n",
      " ('裔移民福利', 1),\n",
      " ('後序幕特朗普', 1),\n",
      " ('墨西哥人', 1),\n",
      " ('宣目的', 1),\n",
      " ('於大道', 1),\n",
      " ('後德', 1)]\n"
     ]
    }
   ],
   "source": [
    "revised_word_frequencies = cleanup_tokens(word_frequencies, english_names)\n",
    "pprint(revised_word_frequencies.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 137\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count: \" + str(len(revised_word_frequencies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Level \n",
    "\n",
    "This seems like overkill for this task but I covered them extensively in my MSc thesis on Hate Speech so I'll suggest it anyway: Word Embeddings. Word Embeddings refer to the set of NLP techniques that are used to map objects (most often words or phrases) into dense vector representations.\n",
    "\n",
    "They enable efficient computation of semantic similarities of words based on their distribution in the underlying language corpus. The core idea is based on the theory of Distributional Hypothesis which states that “words that appear in the same contexts share semantic meaning”. In the domain of Word Embeddings this means\n",
    "that a word will share characteristics with the words that are typically its neighbours in a sentence.\n",
    "\n",
    "The current state of the art method for learning Word Embeddings is [fasttext](https://github.com/facebookresearch/fastText) from Facebook. It offers significant improvements over [word2vec](https://en.wikipedia.org/wiki/Word2vec) as it is able to learn words at a character level.\n",
    "\n",
    "So in this case, if we were to learn Embeddings from Chinese Wikipedia or even rely on a pretrained model then this would be a no brainer. Facebook has made our job easy and they provide pre-trained word embeddings for over 250+ languages [here](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md).\n",
    "\n",
    "The models are quite large so it might not be appropriate to include them here, but I'll just add a small sample and show what would need to be done. I honestly can't remember where exactly I got the pretrained embedding that I include here but I believe it came from this [page](https://sites.google.com/site/rmyeid/projects/polyglot)\n",
    "\n",
    "A non-fancy solution might be to simply use the Edit Distance between words, [Levenshtein Distance](https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm) for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import gensim and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embedding(filename, embedding_type):\n",
    "    \"\"\" Load a fasttext or word2vec embedding\n",
    "    Args:\n",
    "        filename (str)\n",
    "        embedding_type (str): kv:keyedVectors w2v:word2vec\n",
    "    \"\"\"\n",
    "    if embedding_type == \"kv\":\n",
    "        return KeyedVectors.load_word2vec_format(filename, binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_embedding_model = load_embedding(\"wiki.zh_classical.vec\", \"kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore\n",
    "\n",
    "We can start with the Chinese word English, 英文. From the results we can see that the most similar words are other languages, included Latin, Russian, and French.\n",
    "\n",
    "I should note that the similarity is calculated on the cosine distance between the vector representation of 英文 and all other vector representations stored in the model. \n",
    "\n",
    "The model is quite small so it does not contain many examples for us to rely on. Learning such a model over Wikipedia data would allow us to infer the top synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = \"英文\"\n",
    "topn = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('拉丁文', 0.8240991234779358),\n",
       " ('法文', 0.8182531595230103),\n",
       " ('英語', 0.8131850361824036),\n",
       " ('書面', 0.773542582988739),\n",
       " ('希臘文', 0.7686800956726074),\n",
       " ('俄語', 0.7637884616851807),\n",
       " ('教本', 0.7624318599700928),\n",
       " ('交易所', 0.7621933221817017),\n",
       " ('阿拉伯文', 0.7604696750640869),\n",
       " ('拉丁', 0.7577136754989624)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_embedding_model.similar_by_word(word, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of spaCy is that it includes pretrained word vectors by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_top_k_similar(word, k):\n",
    "    \"\"\" Returns the top k similar word vectors from a spacy embedding model.\n",
    "    Args\n",
    "    ----\n",
    "        word (spacy.token): Gensim word embedding model.\n",
    "\n",
    "        k (int): Number of results to return.\n",
    "    \"\"\"\n",
    "    queries = [w for w in word.vocab if not\n",
    "               (word.is_oov or word.is_punct or word.like_num or\n",
    "                word.is_stop or word.lower_ == \"rt\")\n",
    "               and w.has_vector and w.lower_ != word.lower_\n",
    "               and w.is_lower == word.is_lower and w.prob >= -15]\n",
    "\n",
    "    by_similarity = sorted(\n",
    "        queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    cosine_score = [word.similarity(w) for w in by_similarity]\n",
    "    return by_similarity[:k], cosine_score[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english = nlp(\"english\")\n",
    "similar_words, cosine_vals = spacy_top_k_similar(english[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french\n",
      "translation\n",
      "american\n",
      "language\n",
      "dictionary\n",
      "grammar\n",
      "translate\n",
      "wikipedia\n",
      "languages\n",
      "torrent\n"
     ]
    }
   ],
   "source": [
    "for item in similar_words:\n",
    "    print(item.lower_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Level\n",
    "\n",
    "I think that I sort of touched on this in the logic for the correcting the segmentation issues. For Chinese particularly, I think that this can be solved by outline the common patterns that are used to form words. As most Chinese full names are formed with with full characters, if a token that is a common Chinese last name preceeds some other characters tagged as nouns then it might indicate that those characters taken together should form a name.\n",
    "\n",
    "While this by no means an exhaustive list of patterns, I present an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets test a string and see what POS tags we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('蔡', 'Chinese surname'),\n",
       " ('英文', 'other proper noun'),\n",
       " ('小英', 'Chinese given name'),\n",
       " ('英文', 'other proper noun'),\n",
       " ('蔡', 'Chinese surname'),\n",
       " ('博士', 'noun'),\n",
       " ('蔡', 'Chinese surname'),\n",
       " ('總', 'noun'),\n",
       " ('統', 'noun'),\n",
       " ('蔡', 'Chinese surname'),\n",
       " ('女士', 'noun'),\n",
       " ('小英', 'Chinese given name'),\n",
       " ('總', 'noun'),\n",
       " ('統', 'noun'),\n",
       " ('Donald', 'other proper noun'),\n",
       " ('Trump', 'noun'),\n",
       " ('唐', 'Chinese surname'),\n",
       " ('納', 'noun'),\n",
       " ('川普', 'transcribed personal name')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str_2 = \"蔡英文的英文很好\"\n",
    "pynlpir.open()\n",
    "pos_tags = pynlpir.segment(test_str, pos_names='child')\n",
    "pos_tags = [item for item in pos_tags if item[1] in pynlpir_POS_lookup]\n",
    "pynlpir.close()\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['蔡英文', '英文']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nouns = extract_nouns(test_str_2)\n",
    "test_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup the tokens to and keep the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frequencies = Counter()\n",
    "for word in test_nouns:\n",
    "    if word not in token_blacklist:\n",
    "        test_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('蔡英文', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised_test_frequencies = cleanup_tokens(test_frequencies, english_names)\n",
    "revised_test_frequencies.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English version\n",
    "\n",
    "I'm much more familiar with how to do this in English, owing to both the process ready availability of models and my experience with spaCy. The first thing we need to do is collapse the noun phrases, this would give us \"Mr Smith\" as a single token instead of \"Mr\", \"Smith\".\n",
    "\n",
    "Next, we just need to iterate entity property of the object and return entities with a label matching PERSON or whatever we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init doc and collapse phrases\n",
    "doc_1 = nlp(\"Mr Smith is a smith\")\n",
    "doc_2 = nlp(\"Mr Smith is a smith\")\n",
    "for _np in list(doc_1.noun_chunks):\n",
    "    _np.merge(_np.root.tag_, _np.root.lemma_, _np.root.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets check the tags for both\n",
    "\n",
    "We see that doc_1 has collapsed the words to one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "VBZ\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "for token in doc_1:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "NNP\n",
      "VBZ\n",
      "DT\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "for token in doc_2:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract entities, Voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Smith\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_1.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_2.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        print(ent.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
